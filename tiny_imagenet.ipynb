{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D447OLkhMbl"
   },
   "source": [
    "Вспомогательные функции для загрузки данных и правильного формирования тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aU7QBmnVeS0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, sys\n",
    "from urllib.request import urlretrieve\n",
    "import pickle\n",
    "\n",
    "def download_tinyImg200(path,\n",
    "                     url='http://cs231n.stanford.edu/tiny-imagenet-200.zip',\n",
    "                     tarname='tiny-imagenet-200.zip'):\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    urlretrieve(url, os.path.join(path,tarname))\n",
    "    \n",
    "    import zipfile\n",
    "    zip_ref = zipfile.ZipFile(os.path.join(path,tarname), 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "\n",
    "\n",
    "def fix_test_data(data_path, root='tiny-imagenet-200/val'):\n",
    "    root = os.path.join(data_path, root)\n",
    "    with open(os.path.join(root, 'val_annotations.txt')) as f:\n",
    "        annotations = list(map(lambda x: x.split('\\t')[0:2], f.readlines()))\n",
    "        classes = sorted(set(map(lambda x: x[1], annotations)))\n",
    "     \n",
    "    try:\n",
    "        for folder in classes:\n",
    "            os.system('mkdir ' + os.path.join(root, folder))\n",
    "            os.system('mkdir ' + os.path.join(root, folder, 'images'))\n",
    "\n",
    "        for item in annotations:\n",
    "            name, folder = item\n",
    "            new_name = folder + '_' + name\n",
    "            path, new_path = os.path.join(root, 'images', name), os.path.join(root, folder, 'images', new_name)\n",
    "            os.system('cp ' + path + ' ' + new_path)\n",
    "    \n",
    "    except Exception as e:\n",
    "        for folder in classes:\n",
    "            os.system('rm ' + os.path.join(root, folder) + ' -rf')\n",
    "        raise Exception('Something went wrong during copying val images. Check your access rights!')\n",
    "            \n",
    "    os.system('rm ' + os.path.join(root, 'images') + ' -rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixZgLjoL8F04"
   },
   "source": [
    "Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ix8aODJVeQc"
   },
   "outputs": [],
   "source": [
    "data_path = '.'\n",
    "download_tinyImg200(data_path)\n",
    "fix_test_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEtjx91F4h6h"
   },
   "source": [
    "Импортируем нужные библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKS_ECuKHEjH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision, torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fctu8L4YHEjI"
   },
   "source": [
    "Разделим \"tiny-imagenet-200/train\" на тренировочную и валидационную выборки, а \"tiny-imagenet-200/val\" оставим в качестве тренировочной выборки. Для тренировочного и валидационного набора данных будем производить случайный горизонтальный поворот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zyezhn7fjh3y"
   },
   "outputs": [],
   "source": [
    "transform_augment_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_augment_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDbW4JqijpoS"
   },
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transform_augment_train)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/val', transform=transform_augment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cwQB2ptHEjK"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBMDn7keHEjK"
   },
   "outputs": [],
   "source": [
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqXfCtV-JBsN"
   },
   "outputs": [],
   "source": [
    "test_batch_gen = torch.utils.data.DataLoader(test_dataset, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk0ZLdDk5amb"
   },
   "source": [
    "Набор данных содержит 100000 цветных изображений размером 64x64. Всего предсталвенно 200 классов.\n",
    "\n",
    "Для решения задачи классификации я попробовал реализовать небольшую свёрточную сеть. Посмотрим, какое качество получится добиться с реализованной сетью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtRC9klKNGnF",
    "outputId": "cf239e20-b421-4abb-ca0d-c0b6ffbdf328"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('conv_1', nn.Conv2d(3, 64, 3))\n",
    "model.add_module('bn_1', nn.BatchNorm2d(64))\n",
    "model.add_module('relu_1', nn.ReLU())\n",
    "model.add_module('pool_1', nn.MaxPool2d(2, 2))\n",
    "\n",
    "model.add_module('conv_2', nn.Conv2d(64, 128, 3))\n",
    "model.add_module('bn_2', nn.BatchNorm2d(128))\n",
    "model.add_module('relu_2', nn.ReLU())\n",
    "model.add_module('pool_2', nn.MaxPool2d(2, 2))\n",
    "\n",
    "model.add_module('conv_3', nn.Conv2d(128, 256, 3))\n",
    "model.add_module('bn_3', nn.BatchNorm2d(256))\n",
    "model.add_module('relu_3', nn.ReLU())\n",
    "model.add_module('pool_3', nn.MaxPool2d(2, 2))\n",
    "\n",
    "model.add_module('conv_4', nn.Conv2d(256, 512, 3))\n",
    "model.add_module('bn_4', nn.BatchNorm2d(512))\n",
    "model.add_module('relu_4', nn.ReLU())\n",
    "model.add_module('pool_4', nn.MaxPool2d(2, 2))\n",
    "\n",
    "model.add_module('flatten_5', nn.Flatten())\n",
    "\n",
    "model.add_module('linear_6', nn.Linear(2048, 1024))\n",
    "model.add_module(\"dropout_6\", nn.Dropout(p=0.1))\n",
    "model.add_module('relu_6', nn.ReLU())\n",
    "\n",
    "model.add_module('linear_7', nn.Linear(1024, 512))\n",
    "model.add_module(\"droput_7\", nn.Dropout(p=0.1))\n",
    "model.add_module('relu_7', nn.ReLU())\n",
    "\n",
    "model.add_module('dense_8', nn.Linear(512, 200))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoy7OmhHJFdS",
    "outputId": "e91f51b8-5a42-4006-c399-60b881580f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 62, 62]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 62, 62]             128\n",
      "              ReLU-3           [-1, 64, 62, 62]               0\n",
      "         MaxPool2d-4           [-1, 64, 31, 31]               0\n",
      "            Conv2d-5          [-1, 128, 29, 29]          73,856\n",
      "       BatchNorm2d-6          [-1, 128, 29, 29]             256\n",
      "              ReLU-7          [-1, 128, 29, 29]               0\n",
      "         MaxPool2d-8          [-1, 128, 14, 14]               0\n",
      "            Conv2d-9          [-1, 256, 12, 12]         295,168\n",
      "      BatchNorm2d-10          [-1, 256, 12, 12]             512\n",
      "             ReLU-11          [-1, 256, 12, 12]               0\n",
      "        MaxPool2d-12            [-1, 256, 6, 6]               0\n",
      "           Conv2d-13            [-1, 512, 4, 4]       1,180,160\n",
      "      BatchNorm2d-14            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-15            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-16            [-1, 512, 2, 2]               0\n",
      "          Flatten-17                 [-1, 2048]               0\n",
      "           Linear-18                 [-1, 1024]       2,098,176\n",
      "          Dropout-19                 [-1, 1024]               0\n",
      "             ReLU-20                 [-1, 1024]               0\n",
      "           Linear-21                  [-1, 512]         524,800\n",
      "          Dropout-22                  [-1, 512]               0\n",
      "             ReLU-23                  [-1, 512]               0\n",
      "           Linear-24                  [-1, 200]         102,600\n",
      "================================================================\n",
      "Total params: 4,278,472\n",
      "Trainable params: 4,278,472\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 9.92\n",
      "Params size (MB): 16.32\n",
      "Estimated Total Size (MB): 26.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(), (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5159XdRJFbM"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = torch.FloatTensor(X_batch).to(device=device)\n",
    "    y_batch = torch.LongTensor(y_batch).to(device=device)\n",
    "    logits = model.to(device)(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5SFrgT3JFY9",
    "outputId": "7447fa55-0497-413c-a27b-9580061c4e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 15 took 59.039s\n",
      "  training loss (in-iteration): \t4.411328\n",
      "  validation accuracy: \t\t\t15.50 %\n",
      "Epoch 2 of 15 took 58.095s\n",
      "  training loss (in-iteration): \t3.679241\n",
      "  validation accuracy: \t\t\t22.08 %\n",
      "Epoch 3 of 15 took 58.160s\n",
      "  training loss (in-iteration): \t3.365742\n",
      "  validation accuracy: \t\t\t24.62 %\n",
      "Epoch 4 of 15 took 57.539s\n",
      "  training loss (in-iteration): \t3.134482\n",
      "  validation accuracy: \t\t\t28.92 %\n",
      "Epoch 5 of 15 took 58.416s\n",
      "  training loss (in-iteration): \t2.954643\n",
      "  validation accuracy: \t\t\t29.96 %\n",
      "Epoch 6 of 15 took 57.894s\n",
      "  training loss (in-iteration): \t2.798206\n",
      "  validation accuracy: \t\t\t32.00 %\n",
      "Epoch 7 of 15 took 58.303s\n",
      "  training loss (in-iteration): \t2.656835\n",
      "  validation accuracy: \t\t\t33.72 %\n",
      "Epoch 8 of 15 took 56.955s\n",
      "  training loss (in-iteration): \t2.529060\n",
      "  validation accuracy: \t\t\t34.70 %\n",
      "Epoch 9 of 15 took 57.213s\n",
      "  training loss (in-iteration): \t2.411285\n",
      "  validation accuracy: \t\t\t35.87 %\n",
      "Epoch 10 of 15 took 56.660s\n",
      "  training loss (in-iteration): \t2.297968\n",
      "  validation accuracy: \t\t\t36.73 %\n",
      "Epoch 11 of 15 took 59.349s\n",
      "  training loss (in-iteration): \t2.193075\n",
      "  validation accuracy: \t\t\t35.55 %\n",
      "Epoch 12 of 15 took 56.732s\n",
      "  training loss (in-iteration): \t2.090493\n",
      "  validation accuracy: \t\t\t37.06 %\n",
      "Epoch 13 of 15 took 57.466s\n",
      "  training loss (in-iteration): \t1.994979\n",
      "  validation accuracy: \t\t\t36.73 %\n",
      "Epoch 14 of 15 took 56.393s\n",
      "  training loss (in-iteration): \t1.896475\n",
      "  validation accuracy: \t\t\t37.12 %\n",
      "Epoch 15 of 15 took 56.983s\n",
      "  training loss (in-iteration): \t1.798062\n",
      "  validation accuracy: \t\t\t38.02 %\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train(True)\n",
    "    for (X_batch, y_batch) in train_batch_gen:\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    model.train(False)\n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHX4CrJoHEjK"
   },
   "source": [
    "Посчитаем качество на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sXwozunHEjL",
    "outputId": "e95ecde7-c261-4731-a21c-77ba98f7475f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:\t\t36.63 %\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "test_batch_acc = []\n",
    "\n",
    "for X_batch, y_batch in test_batch_gen:\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data\n",
    "    test_batch_acc.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "\n",
    "print(\"test accuracy:\\t\\t{:.2f} %\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp8QKmD28f6M"
   },
   "source": [
    "Как мы видими, результат не самый лучший, но это уже что-то.\n",
    "\n",
    "Чтобы улучшить точность предсказания и не тратить много времени на разработку сети, я взял предварительно обученную свёрточную сеть - ResNet50. Сеть обучалась на датасете задачи классификации картинок ImageNet с 1000 классами. На вход данная сеть принимает цветные картинки размером 224x224. Чтобы мы могли применить ResNet50 к нашей задаче, необходимо изменить размер картинок в нашем датасете. Также, мы не будем обучать сеть заново, а просто изменим последний слой, чтобы вместо предсказания на 1000 классов, сеть производила предсказание на 200 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9gnnGM3Rp_e"
   },
   "outputs": [],
   "source": [
    "transform_augment_train = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_augment_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80dxIKAVjLdb"
   },
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transform_augment_train)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/val', transform=transform_augment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQFFMR56jLC2"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kDKORNIjLAF"
   },
   "outputs": [],
   "source": [
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-rDmj67jK9j"
   },
   "outputs": [],
   "source": [
    "test_batch_gen = torch.utils.data.DataLoader(test_dataset, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4O9gXXN-7Zs"
   },
   "source": [
    "Загрузим ResNet50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ucu-O-2mzfB"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22I0fBH91wRQ"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exbI0WDT_B2H"
   },
   "source": [
    "Отключим обновления весов у всей сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boLC3pli4sn_"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li3wRwp1_SzS"
   },
   "source": [
    "Изменим последний слой и только для него включим возможность обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_QAiPJVkRD"
   },
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 200)\n",
    "    )\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XJ6FyK66bGJ"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = torch.FloatTensor(X_batch).to(device=device)\n",
    "    y_batch = torch.LongTensor(y_batch).to(device=device)\n",
    "    output = model.to(device)(X_batch)\n",
    "    return F.cross_entropy(output, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT6VLVx-suon",
    "outputId": "236d32d3-df11-4ff4-f968-ae65d6403f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 15 took 358.107s\n",
      "  training loss (in-iteration): \t2.841921\n",
      "  validation accuracy: \t\t\t54.93 %\n",
      "Epoch 2 of 15 took 356.986s\n",
      "  training loss (in-iteration): \t1.789467\n",
      "  validation accuracy: \t\t\t58.79 %\n",
      "Epoch 3 of 15 took 357.016s\n",
      "  training loss (in-iteration): \t1.629168\n",
      "  validation accuracy: \t\t\t60.15 %\n",
      "Epoch 4 of 15 took 357.073s\n",
      "  training loss (in-iteration): \t1.551868\n",
      "  validation accuracy: \t\t\t61.25 %\n",
      "Epoch 5 of 15 took 356.687s\n",
      "  training loss (in-iteration): \t1.494007\n",
      "  validation accuracy: \t\t\t61.29 %\n",
      "Epoch 6 of 15 took 356.825s\n",
      "  training loss (in-iteration): \t1.452723\n",
      "  validation accuracy: \t\t\t61.76 %\n",
      "Epoch 7 of 15 took 356.563s\n",
      "  training loss (in-iteration): \t1.416271\n",
      "  validation accuracy: \t\t\t61.76 %\n",
      "Epoch 8 of 15 took 356.819s\n",
      "  training loss (in-iteration): \t1.391402\n",
      "  validation accuracy: \t\t\t62.06 %\n",
      "Epoch 9 of 15 took 356.900s\n",
      "  training loss (in-iteration): \t1.364478\n",
      "  validation accuracy: \t\t\t62.67 %\n",
      "Epoch 10 of 15 took 356.703s\n",
      "  training loss (in-iteration): \t1.337283\n",
      "  validation accuracy: \t\t\t62.69 %\n",
      "Epoch 11 of 15 took 357.460s\n",
      "  training loss (in-iteration): \t1.315236\n",
      "  validation accuracy: \t\t\t62.92 %\n",
      "Epoch 12 of 15 took 356.741s\n",
      "  training loss (in-iteration): \t1.289680\n",
      "  validation accuracy: \t\t\t62.77 %\n",
      "Epoch 13 of 15 took 356.450s\n",
      "  training loss (in-iteration): \t1.271563\n",
      "  validation accuracy: \t\t\t63.19 %\n",
      "Epoch 14 of 15 took 356.381s\n",
      "  training loss (in-iteration): \t1.254013\n",
      "  validation accuracy: \t\t\t62.83 %\n",
      "Epoch 15 of 15 took 356.357s\n",
      "  training loss (in-iteration): \t1.231396\n",
      "  validation accuracy: \t\t\t63.01 %\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.fc.parameters(), lr=1e-4)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train(True)\n",
    "    for (X_batch, y_batch) in train_batch_gen:\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    model.train(False)\n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        output = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = output.max(1)[1].data\n",
    "        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0kkAnWAuPYh",
    "outputId": "77bfa830-64c7-4137-fa77-b107051594dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:\t\t63.25 %\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "test_batch_acc = []\n",
    "\n",
    "for X_batch, y_batch in test_batch_gen:\n",
    "    output = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = output.max(1)[1].data\n",
    "    test_batch_acc.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "\n",
    "print(\"test accuracy:\\t\\t{:.2f} %\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SkElxhc_d37"
   },
   "source": [
    "Без особых усилий удалось увеличить точность предсказания в 1.7 раз. Это довольно неплохой результат :)\n",
    "\n",
    "В качестве более глубокого исследования для достижения лучшего результата можно было увеличить количество \"последних\" слоев; рассмотреть различные функции активации; подобрать шаг обучения в зависимости от изменения значения функции ошибок; увеличить количество эпох; попробовать различные алгоритмы оптимизации; попробовать различные размеры батча; провести аугментацию данных и др."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tiny-imagenet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notebookId": "65f9de61-b446-4240-9ae4-a038dd4de505"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
